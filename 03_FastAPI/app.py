import os
import torch
from transformers import pipeline
from fastapi import FastAPI
import uvicorn
import nest_asyncio
from pyngrok import ngrok
from dotenv import load_dotenv

# .envèª­ã¿è¾¼ã¿
load_dotenv()

# ngrokèªè¨¼
NGROK_TOKEN = os.environ["NGROK_TOKEN"]
ngrok.set_auth_token(NGROK_TOKEN)

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
MODEL_NAME = "google/gemma-2-2b-jpn-it"  # â†ã“ã“ãŒã‚¬ãƒãƒã‚¤ãƒ³ãƒˆ

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ’» ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
print("ğŸŒ€ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
model = pipeline(
    "text-generation",
    model=MODEL_NAME,
    device=0 if device == "cuda" else -1,
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
)
print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼")

# FastAPI appä½œæˆ
app = FastAPI()

# ãƒ«ãƒ¼ãƒˆ
@app.get("/")
def read_root():
    return {"message": "Gemma 2B API server is running!"}

# æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/predict")
def predict(data: dict):
    message = data.get("message", "")
    print(f"ğŸ’¬ å—ã‘å–ã£ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message}")
    output = model(message, max_new_tokens=100)
    generated_text = output[0]["generated_text"]
    return {"response": generated_text}

# å…¬é–‹URLè¡¨ç¤ºï¼†ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
public_url = ngrok.connect(8000)
print(f"âœ¨ APIå…¬é–‹URL: {public_url}")
nest_asyncio.apply()
uvicorn.run(app, host="0.0.0.0", port=8000)
